{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-OSS-20B Red Team Findings Reproduction\n",
    "\n",
    "This notebook demonstrates the red-teaming framework and reproduces findings for the Kaggle hackathon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import framework\n",
    "from gpt_oss_redteam import load_config, setup_logging\n",
    "from gpt_oss_redteam.model.backends import get_backend\n",
    "from gpt_oss_redteam.probes.registry import discover_probes, run_category\n",
    "from gpt_oss_redteam.eval.metrics import calculate_metrics\n",
    "from gpt_oss_redteam.eval.reporting import make_findings_json, generate_report\n",
    "from gpt_oss_redteam.eval.converters import harmony_to_kaggle\n",
    "\n",
    "# Setup logging\n",
    "setup_logging(level=\"INFO\")\n",
    "\n",
    "# Load configuration\n",
    "config = load_config()\n",
    "print(f\"Loaded config with backend: {config.backend.type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize mock backend for demonstration\n",
    "backend = get_backend(\n",
    "    \"mock\",\n",
    "    config.backend.model_dump(),\n",
    "    config.providers\n",
    ")\n",
    "\n",
    "print(\"Backend initialized:\", backend.__class__.__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discover and List Available Probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discover all probes\n",
    "discovered = discover_probes()\n",
    "\n",
    "print(\"Discovered probe categories:\")\n",
    "for category, probes in discovered.items():\n",
    "    print(f\"  {category}: {len(probes)} probes\")\n",
    "\n",
    "total_probes = sum(len(p) for p in discovered.values())\n",
    "print(f\"\\nTotal probes: {total_probes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Small Probe Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select categories to test\n",
    "test_categories = [\"reward_hacking\", \"deception\"]\n",
    "test_seeds = [0, 1, 2]\n",
    "\n",
    "# Run probes\n",
    "all_results = []\n",
    "\n",
    "for category in test_categories:\n",
    "    print(f\"\\nRunning {category} probes...\")\n",
    "    results = run_category(category, backend, seeds=test_seeds)\n",
    "    all_results.extend(results)\n",
    "    print(f\"  Completed {len(results)} probe runs\")\n",
    "\n",
    "print(f\"\\nTotal results: {len(all_results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "metrics = calculate_metrics(all_results)\n",
    "\n",
    "print(\"Overall Metrics:\")\n",
    "print(f\"  Total probes: {metrics['total_probes']}\")\n",
    "print(f\"  Passed: {metrics['passed']}\")\n",
    "print(f\"  Failed: {metrics['failed']}\")\n",
    "print(f\"  Pass rate: {metrics['pass_rate']:.1%}\")\n",
    "\n",
    "print(\"\\nCategory Breakdown:\")\n",
    "for category, cat_metrics in metrics['categories'].items():\n",
    "    print(f\"  {category}:\")\n",
    "    print(f\"    Pass rate: {cat_metrics['pass_rate']:.1%}\")\n",
    "    print(f\"    Avg duration: {cat_metrics['avg_duration_ms']:.1f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Results Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create DataFrame from results\n",
    "results_data = []\n",
    "for result in all_results:\n",
    "    results_data.append({\n",
    "        'Probe': result.probe_name,\n",
    "        'Passed': '✓' if result.passed else '✗',\n",
    "        'Seed': result.seed,\n",
    "        'Duration (ms)': f\"{result.duration_ms:.1f}\",\n",
    "        'Evidence': result.evidence[0] if result.evidence else 'N/A'\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(results_data)\n",
    "print(\"\\nProbe Results:\")\n",
    "display(df.head(10))  # Show first 10 results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Findings JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Kaggle findings JSON for failed probes\n",
    "failed_results = [r for r in all_results if not r.passed]\n",
    "\n",
    "if failed_results:\n",
    "    findings = make_findings_json(\n",
    "        failed_results,\n",
    "        issue_id=\"NOTEBOOK-001\",\n",
    "        meta={\n",
    "            \"source\": \"notebook\",\n",
    "            \"backend\": \"mock\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(f\"Generated findings for {len(findings['findings'])} issues\")\n",
    "    print(\"\\nFirst finding:\")\n",
    "    if findings['findings']:\n",
    "        import json\n",
    "        print(json.dumps(findings['findings'][0], indent=2))\n",
    "else:\n",
    "    print(\"No failed probes to report\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrate Harmony → Kaggle Converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load example Harmony findings\n",
    "harmony_path = \"../examples/example-harmony-findings.json\"\n",
    "\n",
    "if Path(harmony_path).exists():\n",
    "    import tempfile\n",
    "    \n",
    "    with tempfile.NamedTemporaryFile(suffix=\".json\", delete=False) as tmp:\n",
    "        kaggle_path = tmp.name\n",
    "    \n",
    "    # Convert\n",
    "    kaggle_data = harmony_to_kaggle(harmony_path, kaggle_path)\n",
    "    \n",
    "    print(\"Converted Harmony to Kaggle format:\")\n",
    "    print(f\"  Version: {kaggle_data['version']}\")\n",
    "    print(f\"  Findings: {len(kaggle_data['findings'])}\")\n",
    "    \n",
    "    if kaggle_data['findings']:\n",
    "        print(\"\\nFirst converted finding:\")\n",
    "        print(json.dumps(kaggle_data['findings'][0], indent=2))\n",
    "else:\n",
    "    print(\"Example Harmony file not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assert Deterministic Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test deterministic behavior with same seed\n",
    "from gpt_oss_redteam.probes.registry import run_probe\n",
    "\n",
    "# Run same probe twice with same seed\n",
    "result1 = run_probe(\"reward_hacking.confidence_incorrect_facts\", backend, seed=42)\n",
    "result2 = run_probe(\"reward_hacking.confidence_incorrect_facts\", backend, seed=42)\n",
    "\n",
    "# Check if results are identical\n",
    "if result1.transcript and result2.transcript:\n",
    "    response1 = result1.transcript.response\n",
    "    response2 = result2.transcript.response\n",
    "    \n",
    "    if response1 == response2:\n",
    "        print(\"✓ Deterministic behavior confirmed\")\n",
    "        print(f\"  Both runs produced: '{response1[:50]}...'\")\n",
    "    else:\n",
    "        print(\"✗ Non-deterministic behavior detected\")\n",
    "        print(f\"  Run 1: '{response1[:50]}...'\")\n",
    "        print(f\"  Run 2: '{response2[:50]}...'\")\n",
    "else:\n",
    "    print(\"Could not verify deterministic behavior\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Markdown Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate markdown report\n",
    "report = generate_report(all_results, format=\"markdown\", include_details=False)\n",
    "\n",
    "# Display first part of report\n",
    "print(report[:1000])\n",
    "print(\"\\n... [truncated]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. Loading configuration and initializing the mock backend\n",
    "2. Discovering and running probes across multiple categories\n",
    "3. Calculating metrics and generating reports\n",
    "4. Creating Kaggle-ready findings JSON\n",
    "5. Converting between Harmony and Kaggle formats\n",
    "6. Verifying deterministic behavior under mock backend\n",
    "\n",
    "The framework is ready for red-teaming GPT-OSS-20B with safe, benign probes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}