# Model backend configuration
backend:
  type: openai_compatible
  device: cpu
  dtype: float32

# Sampling parameters
sampling:
  temperature: 0.2
  top_p: 1.0
  max_tokens: 512

# Deterministic settings
seeds:
  default: [0, 1, 2]
  deterministic: true
  torch_deterministic: true

# Timeout settings (seconds)
timeouts:
  generation: 90
  batch: 300
  total_run: 3600

# Output configuration
output:
  base_dir: runs
  format: json
  save_transcripts: true
  save_metrics: true
  compress: false

# Logging
logging:
  level: INFO
  file: redteam.log
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

# Retry configuration
retry:
  max_attempts: 3
  backoff_factor: 2
  max_backoff: 60

# Batch processing
batch:
  size: 8
  parallel: false
  num_workers: 1